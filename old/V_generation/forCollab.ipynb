{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)\n",
    "# torch.set_default_dtype(torch.float32)\n",
    "\n",
    "\n",
    "class LossClass:\n",
    "    def __init__(self, reference_image_path, ref_im_size) -> None:\n",
    "        \"\"\"\n",
    "        @param reference_image_path: path to the reference image\n",
    "        @param ref_im_size: size of the reference image (resized to this size)\n",
    "        \"\"\"\n",
    "        # Initialisation of the VGG19 model\n",
    "        self.vgg_cnn = models.vgg19(weights=\"IMAGENET1K_V1\").features.to(device)\n",
    "\n",
    "        self.vgg_cnn.requires_grad_(False)\n",
    "\n",
    "        # initialise l'extraction des couches pour la loss\n",
    "        self.extracted_layers_indexes = [1, 6, 11, 20, 29]\n",
    "        self.layers_weights = [1 / n**2 for n in [64, 128, 256, 512, 512]]\n",
    "\n",
    "        # Contient la réponse des différentes couches de vgg à l'image de référence\n",
    "        self.vgg_outputs = {}\n",
    "\n",
    "        def save_output(name):\n",
    "\n",
    "            # The hook signature\n",
    "            def hook(module, module_in, module_out) -> None:\n",
    "                self.vgg_outputs[name] = module_out\n",
    "\n",
    "            return hook\n",
    "\n",
    "        # le handle est useless\n",
    "        for layer in self.extracted_layers_indexes:\n",
    "            handle = self.vgg_cnn[layer].register_forward_hook(save_output(layer))\n",
    "\n",
    "        # Charge l'image de référence et la prépare (resize, normalisation, etc.)\n",
    "        self.reference_img = prep_img_file(reference_image_path, ref_im_size).to(device)\n",
    "\n",
    "        # Calcul de la matrice de gramm pour chaque couche\n",
    "        self.vgg_cnn(self.reference_img / 0.25)\n",
    "        self.gramm_targets = [\n",
    "            gramm(self.vgg_outputs[key]) for key in self.extracted_layers_indexes\n",
    "        ]\n",
    "\n",
    "    def compute_loss(self, imgs):\n",
    "        # img : batch de 4 images de taille 12x128x128\n",
    "\n",
    "        total_loss = torch.tensor(0.0).to(device)\n",
    "\n",
    "        # Prepare texture data\n",
    "        synth = []\n",
    "        for i in range(4):\n",
    "            synth.append(imgs[i][:3].unsqueeze(0))\n",
    "\n",
    "        # Forward pass using target texture for get activations of selected layers (outputs). Calculate gram Matrix for those activations\n",
    "        for x in synth:\n",
    "            losses = []\n",
    "            self.vgg_cnn(x / 0.25)\n",
    "            synth_outputs = [\n",
    "                self.vgg_outputs[key] for key in self.extracted_layers_indexes\n",
    "            ]\n",
    "            # calcul des loss pour toutes les couches\n",
    "\n",
    "            for activations in zip(\n",
    "                synth_outputs, self.gramm_targets, self.layers_weights\n",
    "            ):\n",
    "                losses.append(gram_loss(*activations).unsqueeze(0))\n",
    "\n",
    "            total_loss = total_loss + torch.cat(losses).sum()\n",
    "        clip_loss = torch.sum(torch.abs(imgs - imgs.clip(-1, 1))) / torch.numel(imgs)\n",
    "        return total_loss + clip_loss\n",
    "\n",
    "    # def compute_loss(self, imgs):\n",
    "    #     \"\"\"\n",
    "    #     Args:\n",
    "    #         imgs: Batch of \"images\" of size B x nb_channels x img_size x img_size, where the 3 fist channels are the RGB channels\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     total_loss = torch.tensor(0.0).to(device)\n",
    "\n",
    "    #     # Extract the RGB channels from the batch of images\n",
    "    #     imgs = imgs[:, :3, :, :]\n",
    "\n",
    "    #     # bach_rgb_channels = []\n",
    "    #     # for i in range(len(imgs)):\n",
    "    #     #     bach_rgb_channels.append(imgs[i][:3].unsqueeze(0))\n",
    "\n",
    "    #     # Forward pass using target texture for get activations of selected layers (outputs). Calculate gram Matrix for those activations\n",
    "\n",
    "    #     self.vgg_cnn(imgs)  # passe le batch d'images dans le réseau de neurones\n",
    "\n",
    "    #     vgg_response = self.vgg_outputs.values()\n",
    "\n",
    "    #     # Calcul de la mse loss pour chaque couche\n",
    "    #     for resp, gramm_target, weight in zip(\n",
    "    #         vgg_response, self.gramm_targets, self.layers_weights\n",
    "    #     ):\n",
    "    #         total_loss = (\n",
    "    #             total_loss + gram_loss(resp, gramm_target).unsqueeze(0) * weight\n",
    "    #         )\n",
    "\n",
    "    #     # total_loss = total_loss + torch.sum(\n",
    "    #     #     torch.abs(imgs - imgs.clip(-1, 1))\n",
    "    #     # ) / torch.numel(imgs)\n",
    "\n",
    "    #     # On calcule la somme de la loss sur les 4 images, c'est chelou, faudrait peut-être faire une moyenne ?\n",
    "\n",
    "    #     return total_loss\n",
    "\n",
    "\n",
    "class RecursiveNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        loss: LossClass,\n",
    "        img_size=128,\n",
    "        img_layer_depth=12,\n",
    "        cpool_size=1024,\n",
    "        learning_rate=2e-4,\n",
    "        bach_size=4,\n",
    "    ):\n",
    "        super(RecursiveNN, self).__init__()\n",
    "\n",
    "        self.loss = loss\n",
    "        self.img_size = img_size\n",
    "        self.img_layer_depth = img_layer_depth\n",
    "        self.cpool_size = cpool_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.bach_size = bach_size\n",
    "\n",
    "        self.ident = torch.tensor(\n",
    "            [[0.0, 0.0, 0.0], [0.0, 1.0, 0.0], [0.0, 0.0, 0.0]]\n",
    "        ).to(device)\n",
    "        self.sobel_x = torch.tensor(\n",
    "            [[-1.0, 0.0, 1.0], [-2.0, 0.0, 2.0], [-1.0, 0.0, 1.0]]\n",
    "        ).to(device)\n",
    "        self.lap = torch.tensor([[1.0, 2.0, 1.0], [2.0, -12, 2.0], [1.0, 2.0, 1.0]]).to(\n",
    "            device\n",
    "        )\n",
    "\n",
    "        self.cn1 = nn.Conv2d(\n",
    "            4 * img_layer_depth, 96, kernel_size=1, padding=0, stride=1\n",
    "        ).to(device)\n",
    "\n",
    "        self.cn2 = nn.Conv2d(\n",
    "            96,\n",
    "            img_layer_depth,\n",
    "            kernel_size=1,\n",
    "            padding=0,\n",
    "            stride=1,\n",
    "            bias=False,  # Semble vraiment être important pour éviter la divergence\n",
    "        ).to(device)\n",
    "\n",
    "        # self.cn1.weight.data.zero_() Surtout pas, augmente énormément le temps de convergence\n",
    "        self.cn2.weight.data.zero_()\n",
    "\n",
    "        # Création de la pool d'images (avec les channels en plus)\n",
    "        self.cpool = torch.rand(\n",
    "            size=(self.cpool_size, self.img_layer_depth, self.img_size, self.img_size),\n",
    "            dtype=torch.float32,\n",
    "            requires_grad=False,\n",
    "        ).to(device)\n",
    "\n",
    "        self.total_training_steps = 0\n",
    "        self.total_params = sum(p.numel() for p in self.parameters())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" \"from the paper\"\"\"\n",
    "        b, ch, h, w = x.shape\n",
    "        filters = torch.stack([self.ident, self.sobel_x, self.sobel_x.T, self.lap]).to(\n",
    "            device\n",
    "        )\n",
    "\n",
    "        y = x.reshape(b * ch, 1, h, w)\n",
    "        y = torch.nn.functional.pad(y, [1, 1, 1, 1], \"circular\")\n",
    "        y = torch.nn.functional.conv2d(y, filters[:, None])\n",
    "        y = y.reshape(b, -1, h, w)\n",
    "\n",
    "        \"\"\"end of paper\"\"\"\n",
    "\n",
    "        out = self.cn2(F.relu(self.cn1(y)))\n",
    "\n",
    "        return out + x\n",
    "\n",
    "    def render(self, it, width, height, save=True):\n",
    "\n",
    "        x = torch.rand(\n",
    "            size=(1, self.img_layer_depth, width, height), dtype=torch.float32\n",
    "        ).to(device)\n",
    "        # remplace un des éléments du batch par du bruit\n",
    "        with torch.no_grad():\n",
    "            for _ in range(it):\n",
    "                x = self(x)\n",
    "\n",
    "        if save:\n",
    "            plt.imsave(\n",
    "                f\"output/{datetime.datetime.now().strftime('%m-%d_%H-%M')}_{it}_iterations.png\",\n",
    "                to_img(x),\n",
    "            )\n",
    "\n",
    "    def start_training(self, nb_steps, debug=False, save_on_interrupt=True):\n",
    "\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        self.train()  # met le modèle en mode entrainement\n",
    "        loss_history = []\n",
    "\n",
    "        # Progress bar\n",
    "        pbar = tqdm(total=nb_steps)\n",
    "\n",
    "        try:\n",
    "            for i in range(nb_steps):\n",
    "\n",
    "                indices = torch.randint(\n",
    "                    low=0, high=self.cpool_size, size=(self.bach_size,)\n",
    "                )\n",
    "                # print(indices)\n",
    "                current_batch = self.cpool[indices]\n",
    "\n",
    "                # remplace une image du batch par du bruit\n",
    "                current_batch[0] = torch.rand(\n",
    "                    size=(self.img_layer_depth, self.img_size, self.img_size),\n",
    "                    dtype=torch.float32,\n",
    "                    requires_grad=False,\n",
    "                ).to(device)\n",
    "                # le clone est nécessaire ? Probablement pas, faut juste remplace le bon dans la pool\n",
    "                current_batch = current_batch.clone().detach()\n",
    "\n",
    "                # On applique itérativement le modèle sur l'image\n",
    "                niter = torch.randint(low=32, high=64, size=(1,))\n",
    "                for _ in range(niter):\n",
    "                    current_batch = self(current_batch)\n",
    "\n",
    "                L = self.loss.compute_loss(current_batch)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    L.backward()\n",
    "                    for p in self.parameters():\n",
    "                        p.grad /= p.grad.norm() + 1e-8  # normalize gradients\n",
    "                    optim.step()\n",
    "                    optim.zero_grad()\n",
    "\n",
    "                # On met à jour la pool d'images\n",
    "                model.cpool[indices] = current_batch.detach()\n",
    "\n",
    "                pbar.set_description(\n",
    "                    f\"\\rstep {i+1} / {nb_steps} | loss: {L.item():.3e} | extremums: [{torch.min(current_batch):.3e}, {torch.max(current_batch):.3e}]\"\n",
    "                )\n",
    "                loss_history.append(L.item())\n",
    "                pbar.update()\n",
    "                self.total_training_steps += 1\n",
    "            if save_on_interrupt:\n",
    "                self.save_weights()\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\ntraining interrupted !\")\n",
    "\n",
    "            if save_on_interrupt:\n",
    "                self.save_weights()\n",
    "            quit(1)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        # save loss history\n",
    "        plt.plot(loss_history)\n",
    "        plt.xlabel(\"Training Step\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss History\")\n",
    "        plt.savefig(\n",
    "            f\"loss_history_{datetime.datetime.now().strftime('%m-%d_%H-%M')}_{self.total_training_steps}_steps.png\"\n",
    "        )\n",
    "\n",
    "    def save_weights(self, dir_path=\"trained_models\") -> None:\n",
    "        def create_directory_if_not_exists(dir_path):\n",
    "            if not os.path.exists(dir_path):\n",
    "                os.mkdir(dir_path)\n",
    "\n",
    "        create_directory_if_not_exists(dir_path)\n",
    "        filename = f\"model_{datetime.datetime.now().strftime('%m-%d_%H-%M')}_{self.total_training_steps}_steps.pth\"\n",
    "        torch.save(self.state_dict(), f\"{dir_path}/{filename}\")\n",
    "\n",
    "    def load_weights(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    l = LossClass(\"grid_0124.jpg\", 128)\n",
    "    model = RecursiveNN(l)\n",
    "\n",
    "    model.start_training(200)\n",
    "\n",
    "    finish = model.render(1, width=128, height=128)\n",
    "\n",
    "    plt.imshow(to_img(finish))\n",
    "    plt.show()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
